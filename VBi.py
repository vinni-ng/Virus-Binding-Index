# -*- coding: utf-8 -*-
"""new_one.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CmS8zyECBTIbllp2CCawfxHxHDuN_9EW
"""


# @title Required Packages
import os
import pandas as pd
from Bio import SeqIO
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from collections import defaultdict
import pickle
import numpy as np
from collections import Counter
import openpyxl
from sklearn.metrics import roc_auc_score, roc_curve

# @title Data curation using Uniprot
# Function 1: Load metadata from an Excel file
def load_metadata(excel_file):
    """
    Load metadata from an Excel file into a DataFrame.
    """
    return pd.read_excel(excel_file)

# Function 2: Load sequences from a FASTA file
def load_sequences(fasta_file):
    """
    Load sequences from a FASTA file into a DataFrame.
    Extract relevant metadata from headers.
    """
    sequences = list(SeqIO.parse(fasta_file, "fasta"))
    sequence_data = []
    for record in sequences:
        # Extract primary ID and additional metadata
        header = record.description  # Full header
        primary_id = header.split('|')[1] if '|' in header else record.id  # Extract ID (e.g., P07612)
        protein_name = header.split(' ')[1] if ' ' in header else "Unknown"  # Extract protein name
        organism = None
        if "OS=" in header:
            organism = header.split("OS=")[1].split("OX=")[0].strip()  # Extract organism name

        # Append parsed data
        sequence_data.append({
            "ID": primary_id,
            "Protein": protein_name,
            "Organism": organism,
            "Sequence": str(record.seq),
            "Length": len(record.seq)
        })

    return pd.DataFrame(sequence_data)

# Function 3: Merge metadata and sequence data
def merge_data(metadata_df, sequences_df):
    """
    Merge metadata and sequence data on Entry and ID.
    """
    merged_df = pd.merge(metadata_df, sequences_df, how="inner", left_on="Entry", right_on="ID")
    return merged_df

# Main script
if __name__ == "__main__":
    # File paths
    excel_file = "./sequences.xlsx"  # Path to the Excel file
    fasta_file = "./sequences.fasta"  # Path to the FASTA file

    # Load metadata and sequences
    metadata_df = load_metadata(excel_file)
    sequences_df = load_sequences(fasta_file)

    # Merge data
    merged_df = merge_data(metadata_df, sequences_df)

    # Save the merged DataFrame to a CSV file
    output_file = "./merged_data.csv"
    merged_df.to_csv(output_file, index=False)

    print(f"Merged data saved to {output_file}")

# @title Data curation from NCBI
# Function 1
def load_metadata(csv_file):
    """
    Load metadata from a CSV file into a DataFrame.
    """
    return pd.read_csv(csv_file)

# Function 2
def load_sequences(fasta_file):
    """
    Load sequences from a FASTA file into a DataFrame.
    """
    sequences = list(SeqIO.parse(fasta_file, "fasta"))
    sequence_data = [{"ID": record.id, "Sequence": str(record.seq), "Length": len(record.seq)} for record in sequences]
    return pd.DataFrame(sequence_data)

# Function 3
def merge_data(metadata_df, sequences_df):
    """
    Merge metadata and sequence data on cleaned ID.
    """
    sequences_df['ID_cleaned'] = sequences_df['ID'].str.split('.').str[0]
    merged_df = pd.merge(metadata_df, sequences_df, how='inner', left_on='Accession', right_on='ID_cleaned')
    return merged_df

# Function 4
def extract_virus_column(df, source_col, target_col):
    """
    Extract virus information from a column using regex.
    """
    df.loc[:, target_col] = df[source_col].str.extract(r'\[(.*?)\]')
    return df

# Function 5
def remove_unwanted_columns(df, columns_to_remove):
    """
    Remove unwanted columns from the DataFrame if they exist.
    """
    return df.drop(columns=[col for col in columns_to_remove if col in df.columns], errors='ignore')

# Function 6
def load_host_mapping_csv(mapping_file):
    """
    Load host-to-Host_agg mapping from a CSV file into a DataFrame.
    """
    return pd.read_csv(mapping_file)

# Function 7
def map_host_to_agg_with_mapping(df, host_col, target_col, mapping_df):
    """
    Map the host column to 'human' or 'non_human' using a mapping DataFrame.
    """
    # Create a dictionary from the mapping DataFrame
    mapping_dict = mapping_df.set_index('Host')['Host_agg'].to_dict()

    # Map the Host values using the dictionary
    df[target_col] = df[host_col].map(mapping_dict)
    return df

# Function 8
def add_human_column(df, host_col, human_col):
    """
    Add a column indicating whether the species is Homo sapiens.
    """
    df[human_col] = df[host_col].apply(lambda x: 1 if x == "human" else 0)
    return df

# Function 9
def drop_duplicates_on_sequence(df):
    """
    Drop duplicate sequences from the DataFrame.
    """
    return df.drop_duplicates(subset=['Sequence'])

# Function 10
def remove_blank_host_rows(df, host_col):
    """
    Remove rows where the host column is blank or NaN.
    """
    return df[df[host_col].notnull() & (df[host_col].str.strip() != '')]

# Main script
if __name__ == "__main__":
    # File paths
    csv_files = ["./sequences.csv"]  # Add more CSV paths as needed
    fasta_files = ["./sequences.fasta"]  # Add more FASTA paths as needed
    mapping_file = "./host_mapping.csv" # File path for the host mapping CSV file

    # Check if CSV and FASTA files are of the same length
    if len(csv_files) != len(fasta_files):
        raise ValueError("The number of CSV files must match the number of FASTA files.")

    # Initialize an empty list to collect DataFrames
    all_data_frames = []

    # Process each CSV and FASTA pair
    for csv_file, fasta_file in zip(csv_files, fasta_files):
        # Load metadata and sequences
        metadata_df = load_metadata(csv_file)
        sequences_df = load_sequences(fasta_file)

        # Merge data
        merged_df = merge_data(metadata_df, sequences_df)

        # Extract virus information
        df_modified = extract_virus_column(merged_df, source_col="GenBank_Title", target_col="Virus")

        # Remove rows with blank Host values
        df_no_blank_host = remove_blank_host_rows(df_modified, host_col='Host')

        # Remove unwanted columns
        unwanted_columns = [
            "Unnamed: 0", "Organism_Name", "GenBank_RefSeq", "Assembly", "Nucleotide", "SRA_Accession",
            "Submitters", "Release_Date", "Isolate", "Genus", "Molecule_type", "Length_x",
            "Genotype", "Segment", "Publications", "Protein", "Geo_Location", "USA",
            "Tissue_Specimen_Source", "Collection_Date", "BioSample", "BioProject", "GenBank_Title", "ID", "Length_y",
            "ID_cleaned"
        ]
        df_col_removed = remove_unwanted_columns(df_no_blank_host, unwanted_columns)

        # Load the mapping
        host_mapping_df = load_host_mapping_csv(mapping_file)

        # Apply the mapping
        df_host_mapped = map_host_to_agg_with_mapping(
            df_col_removed, host_col='Host', target_col='Host_agg', mapping_df=host_mapping_df
        )

        # Add Human column
        df_human_added = add_human_column(df_host_mapped, host_col='Host_agg', human_col='Human')

        # Append the cleaned DataFrame to the list
        all_data_frames.append(df_human_added)

    # Concatenate all data frames into one large DataFrame
    all_df_merged = pd.concat(all_data_frames, ignore_index=True)

    # Remove duplicate sequences
    final_cleaned_df = drop_duplicates_on_sequence(all_df_merged)

    # Save the final cleaned dataset
    output_file = "final_output.csv"
    final_cleaned_df.to_csv(output_file, index=False)
    print(f"Data processing completed. Cleaned data saved to {output_file}.")

# @title Dataset Splitting (Family Wise)
# Load dataset
data = pd.read_csv("./Dataset.csv", encoding='latin-1')

# Iterate over each unique family name in the 'Family' column
for family in data['Family'].unique():
    # Filter the data for the current family
    family_data = data[data['Family'] == family]

    # Save the data into a CSV file named after the family
    filename = f"{family}_data.csv"
    family_data.to_csv(filename, index=False)
    print(f"Saved data for family: {family} to {filename}")

# @title Train test splitting
# Load dataset
try:
    df = pd.read_csv('./Dataset.csv', encoding='latin-1')
except FileNotFoundError:
    print("The specified file was not found.")
    raise

print("Dataset loaded successfully...")

# Check the original dataset
print("Dataset size:", len(df))

print("Loading for filtration...")

# Verify required columns exist
required_columns = ['Host', 'Family']
for col in required_columns:
    if col not in df.columns:
        raise KeyError(f"Column '{col}' is missing in the dataset.")

# Filter out categories with fewer than 2 samples in 'Host_agg', and 'Family'
valid_hosts = df['Host'].value_counts()[df['Host'].value_counts() > 1].index
valid_families = df['Family'].value_counts()[df['Family'].value_counts() > 1].index

df_filtered = df[
    df['Host'].isin(valid_hosts) &
    df['Family'].isin(valid_families)
]

# Check the filtered dataset
print("Filtered dataset size:", len(df_filtered))

# Create a stratification column by combining 'Host_agg', 'Species_agg', and 'Family'
df_filtered = df_filtered.copy()  # Avoid SettingWithCopyWarning
df_filtered['Stratify_col'] = (
    df_filtered['Host'] + "_" +
    df_filtered['Family']
)

# Filter out classes in 'Stratify_col' with fewer than 2 samples
stratify_counts = df_filtered['Stratify_col'].value_counts()
valid_stratify_classes = stratify_counts[stratify_counts > 1].index
df_filtered = df_filtered[df_filtered['Stratify_col'].isin(valid_stratify_classes)]

# Check the filtered dataset size again
print("Filtered dataset size after removing single-sample stratify classes:", len(df_filtered))

# Split dataset
train_df, test_df = train_test_split(
    df_filtered,
    test_size=0.15,  # Adjust as needed
    stratify=df_filtered['Stratify_col'],  # Stratify by the chosen column
    random_state=42  # Ensure reproducibility
)
print("Stratification completed...")

print("Splitting test and train completed...")

# Verify the split
print("Train set size:", len(train_df))
print("Test set size:", len(test_df))
print("Train set distribution:")
print(train_df['Stratify_col'].value_counts())
print("Test set distribution:")
print(test_df['Stratify_col'].value_counts())

# @title k-mer size optimization
# Modify the defaultdict setup to avoid using lambda inside it
class FamilyKmers:
    def __init__(self):
        self.homo = defaultdict(set)  # Store unique k-mers as sets
        self.non_homo = defaultdict(set)  # Store unique k-mers as sets

def generate_kmers(sequence, k):
    """Generate overlapping k-mers from a sequence."""
    return [sequence[i:i+k] for i in range(len(sequence) - k + 1)]

def filter_kmers(kmers):
    """Filter k-mers by excluding those containing 'X'."""
    return [kmer for kmer in kmers if 'X' not in kmer]

def filter_frequent_kmers(kmer_counts, min_occurrence=2):
    """
    Filter k-mers based on their occurrence across sequences.

    Args:
        kmer_counts (dict): A dictionary with k-mers as keys and their counts across sequences as values.
        min_occurrence (int): The minimum number of sequences in which a k-mer must appear to be included.

    Returns:
        list: Filtered list of k-mers that appear in more than the specified minimum number of sequences.
    """
    return [kmer for kmer, count in kmer_counts.items() if count >= min_occurrence]


def process_kmers_by_family(dataframe, k_range):
    """
    Process the dataframe and generate k-mers for each family,
    separating homo (human) and non-homo (non-human) k-mers.
    Removes k-mers that are found in both categories and filters k-mers
    that appear in more than one sequence.
    """
    family_kmers = defaultdict(FamilyKmers)
    kmer_counts = defaultdict(lambda: defaultdict(int))  # Track k-mer occurrences across sequences

    print("Started processing k-mers...")

    # Iterate through each row and extract sequences for homo and non-homo
    for _, row in dataframe.iterrows():
        sequence = row['Sequence']
        family = row['Family']
        human = row['Plant']
        # Extract k-mers for the given range
        for k in k_range:
            kmers = generate_kmers(sequence, k)
            kmers = filter_kmers(kmers)  # Exclude k-mers containing 'X'

            # Count k-mer occurrences across sequences
            for kmer in kmers:
                kmer_counts[family][kmer] += 1

            # Separate into homo and non-homo
            if human == 1:  # Homo (human)
                for kmer in kmers:
                    family_kmers[family].homo[kmer] = 1
            else:  # Non-homo (non-human)
                for kmer in kmers:
                    family_kmers[family].non_homo[kmer] = 1

    print("Finished generating k-mers. Removing overlapping and infrequent k-mers...")

    # Remove overlapping k-mers and filter based on frequency
    for family, kmers_obj in family_kmers.items():
        homo_kmers_set = set(kmers_obj.homo.keys())
        non_homo_kmers_set = set(kmers_obj.non_homo.keys())

        # Find overlapping k-mers
        overlapping_kmers = homo_kmers_set & non_homo_kmers_set

        # Remove overlapping k-mers from both categories
        for kmer in overlapping_kmers:
            del kmers_obj.homo[kmer]
            del kmers_obj.non_homo[kmer]

        # Filter k-mers that appear in more than one sequence
        filtered_homo = filter_frequent_kmers(kmer_counts[family])
        filtered_non_homo = filter_frequent_kmers(kmer_counts[family])

        # Retain only the filtered k-mers in each category
        kmers_obj.homo = {kmer: 1 for kmer in filtered_homo if kmer in kmers_obj.homo}
        kmers_obj.non_homo = {kmer: 1 for kmer in filtered_non_homo if kmer in kmers_obj.non_homo}

    print("Overlapping and infrequent k-mers removed.")
    return family_kmers


def plot_kmers_distribution(family_kmers, k_range):
    """
    Plot the distribution of unique k-mers for each family.
    The plot will show the number of unique homo and non-homo k-mers for each family.
    Additionally, it will print the k-mer size with the maximum count for each family.
    """
    for family, kmers_obj in family_kmers.items():
        homo_counts = []
        non_homo_counts = []

        # For tracking unique k-mers for each size
        unique_homo_kmers = defaultdict(set)
        unique_non_homo_kmers = defaultdict(set)

        # Count unique k-mers for each size in the specified range
        for k in k_range:
            # Get the unique k-mers for size k
            homo_kmers = [kmer for kmer in kmers_obj.homo if len(kmer) == k]
            non_homo_kmers = [kmer for kmer in kmers_obj.non_homo if len(kmer) == k]

            # Add k-mers to the unique sets
            unique_homo_kmers[k] = set(homo_kmers)
            unique_non_homo_kmers[k] = set(non_homo_kmers)

            # Get the count of unique k-mers for size k
            homo_count = len(unique_homo_kmers[k])
            non_homo_count = len(unique_non_homo_kmers[k])

            homo_counts.append(homo_count)
            non_homo_counts.append(non_homo_count)

            # Print the number of unique k-mers for each size
            print(f"For family {family}, Homo (human) k-mers of size {k}: {homo_count} unique k-mers")
            print(f"For family {family}, Non-Homo (non-human) k-mers of size {k}: {non_homo_count} unique k-mers")

        # Determine the k-mer size with the maximum count for Homo and Non-Homo
        max_homo_k = k_range[homo_counts.index(max(homo_counts))] if homo_counts else None
        max_non_homo_k = k_range[non_homo_counts.index(max(non_homo_counts))] if non_homo_counts else None

        print(f"\nFor family {family}, the k-mer size with the max count for Homo (human): {max_homo_k} with {max(homo_counts)} unique k-mers")
        print(f"For family {family}, the k-mer size with the max count for Non-Homo (non-human): {max_non_homo_k} with {max(non_homo_counts)} unique k-mers\n")

        # Create the plot for unique k-mers
        plt.figure(figsize=(10, 6))
        plt.plot(k_range, homo_counts, label=f'HOMO ({family})', color='blue', marker='o', linestyle='-', alpha=0.7)
        plt.plot(k_range, non_homo_counts, label=f'non-HOMO ({family})', color='orange', marker='o', linestyle='-', alpha=0.7)

        # Adding labels and title
        plt.title(f'Unique K-mer Distribution for {family} Family')
        plt.xlabel('K-mer Size')
        plt.ylabel('Unique K-mer Count')
        plt.legend()
        plt.grid(True, linestyle='--', alpha=0.5)
        plt.show()

def save_kmers_up_to_max_count(family_kmers, k_range):
    """
    Save only up to the k-mer size where the maximum count was found for each family.
    """
    family_kmers_to_save = defaultdict(FamilyKmers)

    # Save only up to the k-mer size where the maximum count was found for each family
    for family, kmers_obj in family_kmers.items():
        # For Homo (human)
        homo_counts = [len([kmer for kmer in kmers_obj.homo if len(kmer) == k]) for k in k_range]
        max_homo_k = k_range[homo_counts.index(max(homo_counts))] if homo_counts else None

        # Save k-mers up to the max observed count for Homo (human)
        if max_homo_k:
            for k in range(3, max_homo_k + 1):
                for kmer in kmers_obj.homo:
                    if len(kmer) == k:
                        family_kmers_to_save[family].homo[kmer] = 1

        # For Non-Homo (non-human)
        non_homo_counts = [len([kmer for kmer in kmers_obj.non_homo if len(kmer) == k]) for k in k_range]
        max_non_homo_k = k_range[non_homo_counts.index(max(non_homo_counts))] if non_homo_counts else None

        # Save k-mers up to the max observed count for Non-Homo (non-human)
        if max_non_homo_k:
            for k in range(3, max_non_homo_k + 1):
                for kmer in kmers_obj.non_homo:
                    if len(kmer) == k:
                        family_kmers_to_save[family].non_homo[kmer] = 1

    # Save the family_kmers_to_save dictionary to a pickle file
    with open('family_kmers_to_save.pkl', 'wb') as f:
        pickle.dump(family_kmers_to_save, f)

    print("K-mers up to max count size have been saved to 'family_kmers_to_save.pkl'")


# Example usage:
# Assuming you have a dataframe `train_df` with the required columns: 'Sequence', 'Human', and 'Family'
# Define the k-mer size range
k_range = range(3, 10)  # From size 3 to 10

# Process k-mers by family
family_kmers = process_kmers_by_family(train_df, k_range)

# Plot the unique k-mer distributions for each family
plot_kmers_distribution(family_kmers, k_range)

# Save the k-mers up to the maximum count size
save_kmers_up_to_max_count(family_kmers, k_range)

# Optionally, save the family_kmers dictionary to a pickle file
with open('family_kmers_to_save.pkl', 'wb') as f:
    pickle.dump(family_kmers, f)

print("Unique K-mers up to max count size have been saved to 'family_kmers_to_save.pkl'")

# @title View the kmer pickled file
# Load the pickle file
with open('family_kmers_to_save.pkl', 'rb') as f:
    family_kmers_to_save = pickle.load(f)

# View the keys (families) in the data
print("Families in the pickle file:")
for family in family_kmers_to_save.keys():
    print(family)

# Optionally view the k-mers for a specific family
family_to_view = input("Enter the family name to view its k-mers (or press Enter to skip): ").strip()
if family_to_view in family_kmers_to_save:
    print(f"\nHomo (human-like) k-mers for family '{family_to_view}':")
    print(list(family_kmers_to_save[family_to_view].homo.keys())[:10000])  # Print only first 10 for brevity

    print(f"\nNon-Homo (non-human-like) k-mers for family '{family_to_view}':")
    print(list(family_kmers_to_save[family_to_view].non_homo.keys())[:10000])  # Print only first 10 for brevity
else:
    print("No specific family selected or family not found.")

# If you want to print the entire structure, uncomment below
# import pprint
# pprint.pprint(family_kmers_to_save)

# @title Testing

def load_kmers(pickle_file):
    """Load kmers from the trained data."""
    with open(pickle_file, "rb") as file:
        family_kmers_to_save = pickle.load(file)

    # Convert homo and non-homo k-mer lists into sets for fast lookup
    for family, trained_family in family_kmers_to_save.items():
        trained_family.homo = set(trained_family.homo)
        trained_family.non_homo = set(trained_family.non_homo)

    return family_kmers_to_save

def generate_kmers(sequence, k):
    """Generate k-mers of length k from a given sequence."""
    return [sequence[i:i + k] for i in range(len(sequence) - k + 1) if 'X' not in sequence[i:i + k]]

def count_kmers(test_df, family_kmers_to_save):
    """Count filtered k-mers, HOMO and NON-HOMO k-mers for the correct family and add respective columns."""
    filtered_kmer_counts = []
    homo_counts = []
    non_homo_counts = []

    for _, row in test_df.iterrows():
        family = row['Family']
        sequence = row['Sequence']

        if family not in family_kmers_to_save:
            filtered_kmer_counts.append(0)
            homo_counts.append(0)
            non_homo_counts.append(0)
            continue

        trained_family = family_kmers_to_save[family]

        # Generate k-mers for lengths from 3 to 10
        filtered_kmers = [kmer for k in range(3, 11) for kmer in generate_kmers(sequence, k)]
        filtered_kmer_counts.append(len(filtered_kmers))

        # Count how many k-mers match the "homo" and "non-homo" sets
        homo_count = sum(1 for kmer in filtered_kmers if kmer in trained_family.homo)
        non_homo_count = sum(1 for kmer in filtered_kmers if kmer in trained_family.non_homo)

        homo_counts.append(homo_count)
        non_homo_counts.append(non_homo_count)

    # Add the counts to the DataFrame
    test_df['Filtered_Kmer_Count'] = filtered_kmer_counts
    test_df['HOMO_Count'] = homo_counts
    test_df['NON_HOMO_Count'] = non_homo_counts

    return test_df

def add_binds_column(test_df):
    """Add Binds column: 1 if HOMO_Count > NON_HOMO_Count, else 0."""
    test_df['Binds'] = (test_df['HOMO_Count'] > test_df['NON_HOMO_Count']).astype(int)
    return test_df

# Main Execution
pickle_file = "family_kmers_to_save.pkl"

# Load k-mer data
family_kmers_to_save = load_kmers(pickle_file)

# Process the test data and count k-mers
test_df = count_kmers(test_df, family_kmers_to_save)

# Add 'Binds' column
test_df = add_binds_column(test_df)

# Clean the dataframe by keeping only relevant columns
columns_to_keep = ['Accession', 'Species', 'Family', 'Country', 'Host', 'Sequence', 'Virus',
                  'Plant', 'HOMO_Count',
                   'NON_HOMO_Count', 'Binds']

test_df_cleaned = test_df[columns_to_keep]

# Save the cleaned dataframe to a new CSV file
test_df_cleaned.to_csv("results.csv", index=False)
print("Cleaned test dataframe saved to 'results.csv'")

# @title ROC_AUC

# Assuming `test_df` contains the 'Human' and 'Binds' columns
y_true = test_df['Plant']  # True labels
y_pred = test_df['Binds']  # Predicted labels

# Calculate ROC AUC score
roc_auc = roc_auc_score(y_true, y_pred)
print(f"ROC AUC Score: {roc_auc}")

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_true, y_pred)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Random classifier line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()

# @title ROC_AUC for each family
# Dictionary to store AUC scores for each family
family_auc_scores = {}

# Iterate over each family in the dataset
for family, df in test_df.groupby("Family"):
    if len(df["Plant"].unique()) < 2:  # Skip families that don't have both 0 and 1
        print(f"Skipping Family '{family}' (only one class present)")
        continue

    y_true = df["Plant"]  # True labels
    y_pred = df["Binds"]  # Predicted labels

    # Compute ROC AUC score
    roc_auc = roc_auc_score(y_true, y_pred)
    family_auc_scores[family] = roc_auc

    # Compute ROC curve
    fpr, tpr, _ = roc_curve(y_true, y_pred)

    # Plot ROC curve for each family
    plt.figure(figsize=(6, 4))
    plt.plot(fpr, tpr, color='blue', label=f'AUC = {roc_auc:.2f}')
    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Random classifier line
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve - {family}')
    plt.legend(loc="lower right")
    plt.show()

# Print all AUC scores
print("\nROC AUC Scores for Each Family:")
for family, auc in family_auc_scores.items():
    print(f"Family: {family}, AUC: {auc:.2f}")

# @title Checking false positives and false negatives
def calculate_false_positives_and_false_negatives(test_df):
    """
    Calculate the number of False Positives (FP) and False Negatives (FN) based on the 'Human' and 'Binds' columns.
    Also, return the accession IDs for False Positives.
    """
    false_positives = 0
    false_negatives = 0
    false_positive_accessions = []

    # Iterate through each row of the test dataframe
    for idx, row in test_df.iterrows():
        true_label = row['Plant']
        predicted_label = row['Binds']
        accession_id = row.get('Accession', None)  # Assuming there's an 'Accession_ID' column

        # False Positive: Predicted 1 (human-like), but actual label is 0 (non-human)
        if predicted_label == 1 and true_label == 0:
            false_positives += 1
            if accession_id:
                false_positive_accessions.append(accession_id)

        # False Negative: Predicted 0 (non-human-like), but actual label is 1 (human)
        if predicted_label == 0 and true_label == 1:
            false_negatives += 1

    return false_positives, false_negatives, false_positive_accessions

# Example usage:
false_positives, false_negatives, false_positive_accessions = calculate_false_positives_and_false_negatives(test_df)

print(f"False Positives: {false_positives}")
print(f"False Negatives: {false_negatives}")
print(f"False Positive Accession IDs: {false_positive_accessions}")

# @title Checking on single sequence
def load_kmers(pickle_file):
    """Load kmers from the trained data."""
    with open(pickle_file, "rb") as file:
        family_kmers_to_save = pickle.load(file)

    # Convert homo and non-homo k-mer lists into sets for fast lookup
    for family, trained_family in family_kmers_to_save.items():
        trained_family.homo = set(trained_family.homo)
        trained_family.non_homo = set(trained_family.non_homo)

    return family_kmers_to_save

def generate_kmers(sequence, k_min=3, k_max=10):
    """Generate k-mers of sizes k_min to k_max, avoiding k-mers with 'X'."""
    kmers = []
    for k in range(k_min, k_max + 1):
        kmers.extend([sequence[i:i + k] for i in range(len(sequence) - k + 1) if 'X' not in sequence[i:i + k]])
    return kmers

def identify_family_and_classification(sequence, family_kmers_to_save):
    """Identify the family and classify as HOMO or NON-HOMO."""
    kmer_counts = Counter()
    homo_counts = Counter()
    non_homo_counts = Counter()

    # Generate k-mers
    kmers = generate_kmers(sequence)

    # Compare with known families
    for family, trained_family in family_kmers_to_save.items():
        kmer_matches = sum(1 for kmer in kmers if kmer in trained_family.homo or kmer in trained_family.non_homo)
        if kmer_matches > 0:
            kmer_counts[family] = kmer_matches
            homo_counts[family] = sum(1 for kmer in kmers if kmer in trained_family.homo)
            non_homo_counts[family] = sum(1 for kmer in kmers if kmer in trained_family.non_homo)

    # Determine the most probable family
    predicted_family = kmer_counts.most_common(1)[0][0] if kmer_counts else "Unknown"

    # Determine classification
    classification = "Unknown"
    if predicted_family in homo_counts and predicted_family in non_homo_counts:
        if homo_counts[predicted_family] > non_homo_counts[predicted_family]:
            classification = "Yes there is a chance of binding"
        else:
            classification = "No Chance for Binding"

    return predicted_family, classification

def process_fasta(fasta_file, family_kmers_to_save):
    """Process a FASTA file and print the results."""
    for record in SeqIO.parse(fasta_file, "fasta"):
        sequence_id = record.id
        sequence = str(record.seq)

        predicted_family, classification = identify_family_and_classification(sequence, family_kmers_to_save)

        # Print the results
        print("-" * 80)
        print(f"Sequence ID: {sequence_id}")
        print(f"Sequence: {sequence}")
        print(f"Predicted Family: {predicted_family}")
        print(f"Classification: {classification}")
        print("-" * 80)

# Main Execution
pickle_file = "family_kmers_to_save.pkl"
fasta_file = "sequences.fasta"

# Load trained k-mer data
family_kmers_to_save = load_kmers(pickle_file)

# Process the FASTA file and print results
process_fasta(fasta_file, family_kmers_to_save)

